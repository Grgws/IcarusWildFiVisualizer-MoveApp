
import os
import gpxpy
import pandas as pd
import numpy as np

from . import dbaCalculator as dba


def importWildFiData(   wildFiData,
                        metaDataFile=None, 
                        gpxDataDir=None, 

                        dropDataWithoutMeta=True, 
                        mergeGPSPoints = True, 
                        parseAcc = True, 

                        convertToTZ = None,
                        ):
    """ Imports wildFiData from csv files or just takes a pandas DataFrame and
        does some operations like GPS and PROX merging or ACC parsing on it.

    Args:
        wildFiData (str or pd.DataFrame): Either path to a directory containing the csv files generated by the wildFiDecoder or a pandas DataFrame containing the data
        metaDataFile (str): Path of file containing the metadata for the tags. Data is merged via tagId column
        gpxDataDir (str, optional): Directory containing reference gpx files. Defaults to None.
        
        dropDataWithoutMeta (bool, optional): If true, data of tags that are not in metaDataFile is dropped. Defaults to True.
        mergeGPSPoints (bool, optional): If true, gps points are merged with corresponding proximity points before each gps point. Defaults to True.
        parseAcc (bool,optional): If true, accelerometer data is parsed into list of x,y,z np arrays. Defaults to True.

        convertToTZ (str, optional): If set, the time column is converted to the specified timezone. Defaults to None.

    Returns:
        pandas.DataFrame: Dataframe containing all imported data
    """
    
    # Read data from csv files if wildFiData is path to directory
    if isinstance(wildFiData,str) and os.path.exists(wildFiData):
        df = readRawWildFiData(wildFiData)
    elif isinstance(wildFiData,pd.DataFrame):
        df = wildFiData
    else:
        raise TypeError("wildFiData must be either the path to a directory containing the csv files generated by the wildFiDecoder or a pandas DataFrame containing the raw csv data")


    # Convert utc seconds to native datetime (fillna to also use prefilled time of gpx files)
    df["time"] = pd.to_datetime(df['utcTimestamp'],unit='s',utc=True)
    if convertToTZ:
        df["time"] = df["time"].dt.tz_convert(convertToTZ)

    # Merge with metaDataFile ...
    if metaDataFile:
        metaDf = pd.read_csv(metaDataFile, dtype={"tagId":"category"})
        # ... ignore Data of tags that are not in metaDataFile (how="inner") if dropDataWithoutMeta
        df = pd.merge(df,metaDf,on="tagId",how=("inner" if dropDataWithoutMeta else "left"))


    if(gpxDataDir):
        df = pd.concat([df,importGpxData(gpxDataDir)])
        

    # Sort by time for correct time series plotting
    df.sort_values(by=["time","tagId"],inplace=True, ascending=True)

    
    #Merge GPS and Prox data
    if mergeGPSPoints:
        df.dropna(subset="time",inplace=True)

        df = pd.merge_asof(df[df.lat.isna()], df[df.lat.notna()], 
                            on="time",by="tagId",
                            suffixes=("","_GPS"),
                            direction="forward",
                            tolerance=pd.Timedelta('5min'))
        df["lat"] = df["lat_GPS"]
        df["lon"] = df["lon_GPS"]
        df["ttfSeconds"] = df["ttfSeconds_GPS"]
        df["hdop"] = df["hdop_GPS"]
        df["gpsTimeDiff"] = df["utcTimestamp_GPS"]-df["utcTimestamp"]
        df.drop(df.filter(regex="_GPS$").columns,axis=1,inplace=True)


    # Add additional metrics
    df["proxIdBurst"] = df["proxIdBurst"].astype(str).replace("nan","") # Replace nan with empty string for more consistent format 
    df["proxRssiBurst"] = df["proxRssiBurst"].astype(str).replace("nan","")
    df["proxCount"] = (df.loc[df.proxIdBurst.notna(),"proxIdBurst"].str.split()).str.len().fillna(0)
    df["proxIdBurstSorted"] = (df.loc[df.proxIdBurst.notna() & (df.proxIdBurst != "nobody"),"proxIdBurst"].str.split()).apply(lambda x: sorted(x))
    # df["dataType"] = ["GPS" if x else "PROX" for x in df.lat.notna()]
    df["GPS"] = df.lat.notna()


    # Parse acc burst data
    if parseAcc:
        accNotNa = df.accInGBurst.notna()

        df["accInGBurst"] = df.loc[accNotNa,"accInGBurst"].apply(parseAccBurstData)

        dbaSeries = df.loc[accNotNa,"accInGBurst"].apply(dba.calcDBA)
        df["VeDBA"] = dbaSeries.apply(dba.calcVeDBA)
        df["VeDBAMean"] = df.loc[accNotNa,"VeDBA"].apply(np.mean)
        df["ODBA"] = dbaSeries.apply(dba.calcODBA)
        df["ODBAMean"] = df.loc[accNotNa,"ODBA"].apply(np.mean)

        # Convert each Nx3 numpy array to list of 3 numpy arrays [x,y,z] 
        # (Doing this instead of just storing the 3xN array is to make it
        # compatible with parquet storage format used for caching, since multidim arrays are
        # not supported there)
        df["accInGBurst"] = df.loc[accNotNa,"accInGBurst"]\
                                    .apply(lambda accBurst: list(accBurst.transpose((1,0))))


    print(f"Dataset size: {df.shape}")

    return df

def importGpxData(gpxDataDir):
    """Import coordinate data from gpx files in gpxDataDir as pandas df"""

    #create list of gpx files in gpxDataDir
    files = [f for f in os.listdir(gpxDataDir) if os.path.isfile(os.path.join(gpxDataDir,f)) and f.endswith(".gpx")]

    # Parse all files
    gpxs = []
    for file in files:
        with open(os.path.join(gpxDataDir,file)) as f:
            gpxs.append(gpxpy.parse(f))

    # Convert to a dataframe one point at a time.
    gpxDfs = []
    points = []
    for gpx,file in zip(gpxs,files):
        for segment in gpx.tracks[0].segments:
            for p in segment.points:
                points.append({
                    'tagId': file,
                    'tagDesc': file,
                    'time': p.time,
                    'lat': p.latitude,
                    'lon': p.longitude,
                    'hdop': p.horizontal_dilution
                })

    
    gpxDf = pd.DataFrame.from_records(points)
    gpxDf.time = pd.to_datetime(gpxDf.time,format="%Y-%m-%dT%H:%M:%SZ",utc=True)
    return gpxDf

def readRawWildFiData(wildFiDataDir):
    """Recursively read all csv files in wildFiDataDir into dataframe"""
    # Create list of csv files in wildFiDataDir
    files = []
    for dirpath, dirnames, filenames in os.walk(wildFiDataDir):
        for filename in [f for f in filenames if f.endswith(".csv")]:
            files.append(os.path.join(dirpath,filename))

    # files = [f for f in os.listdir(wildFiDataDir) if os.path.isfile(os.path.join(wildFiDataDir,f)) and f.endswith(".csv")]


    # Read all files into list of dataframes
    dfs = [pd.read_csv(f) for f in files]
    
    print(f"Read {len(dfs)} files.")

    # Add Filnames to data
    for iDf, f in zip(dfs,files):
        iDf["filename"] = f

    # Combine all dataframes into one
    return pd.concat(dfs).reset_index(names=["csvIndex"])

def parseAccBurstData(accBurst):
    """Returns Nx3 numpy array parsed from space delimited accInGBurst string (x y z x y z ...)"""
    accBurstParsed = np.fromstring(accBurst,dtype=np.single,sep=" ")
    try:
        return np.reshape(accBurstParsed ,(-1,3),order='C')
    except ValueError:
        print(f"Error parsing acc burst data (incomplete burst? size/3: {accBurstParsed.size/3})")# accBurst: {accBurst}")
        return np.array([[]])


def getAccBurstDf(row, accFreq=50, offsetMillis = 20):
    """Returns df with x, y, z, VeDBA and ODBA columns over time index parsed from
        accInGBurst list of [x,y,z] 1d numpy arrayd and time columns of row"""
    # Get list of parsed acc burst data (convert from list of 1d numpy arrays to 2d numpy array)
    accBurstParsed = np.dstack(row.accInGBurst)[0]
    # Append VeDBA and ODBA columns 
    accBurstParsed = np.append(accBurstParsed,np.reshape(row.ODBA,(-1,1)),axis=1)
    accBurstParsed = np.append(accBurstParsed,np.reshape(row.VeDBA,(-1,1)),axis=1)
    # Generate time index and return df
    period = 1/accFreq*1000
    time = np.arange(offsetMillis,stop=accBurstParsed.shape[0]*period+offsetMillis,step=period)
    return  pd.DataFrame(accBurstParsed,columns=["x","y","z","ODBA","VeDBA"],index=time)
